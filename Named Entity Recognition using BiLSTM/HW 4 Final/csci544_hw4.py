# -*- coding: utf-8 -*-
"""CSCI544_HW4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mGwFNklHehTLA14K0vFdqvetbsDnIvvc
"""

# from google.colab import drive
# drive.mount('/content/drive')

#Importing necessary libraries 
import numpy as np
import pandas as pd
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence
from torch.utils.data import Dataset, DataLoader
from torch.optim.lr_scheduler import StepLR
import random
torch.manual_seed(0)
random.seed(0)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device



def read_and_prep_file(path):
    trn_X, trn_Y = [], []
    x, y = [], []
    first = True
    
    with open(path, 'r') as f:
        for line in f.readlines():
            if len(line) > 2:
                id, word, ner_tag = line.strip().split(" ")
                if id == '1' and not first:
                    trn_X.append(x)
                    trn_Y.append(y)
                    x, y = [], []
                first = False
                x.append(word)
                y.append(ner_tag)

    trn_X.append(x)
    trn_Y.append(y)
    return trn_X, trn_Y

def read_and_prep_test_file(path):
    tst_x = []
    x = []
    first = True

    with open(path, 'r') as f:
        for line in f.readlines():
            if len(line) > 1:
                id, word = line.strip().split(" ")
                if id == '1' and not first:
                    tst_x.append(x)
                    x = []
                first = False
                x.append(word)

    tst_x.append(x)
    return tst_x


X_train, Y_train = read_and_prep_file('drive/My Drive/CSCI544/hw4/data/train')
X_val, Y_val= read_and_prep_file('drive/My Drive/CSCI544/hw4/data/dev')
X_test = read_and_prep_test_file('drive/My Drive/CSCI544/hw4/data/test')

print(len(X_train), len(Y_train))
print(len(X_val), len(Y_val))
print(len(X_test))

for i in range(10):
  print(X_train[i])

class BiLSTM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,
                 bidirectional, dropout_val, tag_size):
        super(BiLSTM, self).__init__()
        
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, num_layers=lstm_layers, 
                            batch_first=True, bidirectional=bidirectional)
        self.fc = nn.Linear(hidden_dim*2 if bidirectional else hidden_dim, linear_out_dim)
        self.dropout = nn.Dropout(dropout_val)
        self.elu = nn.ELU(alpha=0.01)
        self.classifier = nn.Linear(linear_out_dim, tag_size)
        
    def forward(self, inputs, lengths):
        embeddings = self.embedding(inputs)   
        packed_embeddings = nn.utils.rnn.pack_padded_sequence(embeddings, lengths, 
                                                              batch_first=True, 
                                                              enforce_sorted=False)
        packed_output, _ = self.LSTM(packed_embeddings)
        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        output = self.dropout(output)
        output = self.fc(output)
        output = self.elu(output)
        output = self.classifier(output)
        return output







class BiLSTM_DataLoader(Dataset):
    def __init__(self, x, y):
        self.x = x
        self.y = y

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        x_instance = torch.tensor(self.x[index])
        y_instance = torch.tensor(self.y[index])
        return x_instance, y_instance

class BiLSTM_TestLoader(Dataset):
    def __init__(self, x):
        self.x = x

    def __len__(self):
        return len(self.x)

    def __getitem__(self, index):
        x_instance = torch.tensor(self.x[index])
        return x_instance

class CustomCollator(object):
    def __init__(self, vocab, label):
        self.params = vocab
        self.label = label
    
    def __call__(self, batch):
        xx, yy = zip(*batch)
        x_len = [len(x) for x in xx]
        y_len = [len(y) for y in yy]
        batch_max_len = max([len(s) for s in xx])
        batch_data = self.params['<PAD>'] * np.ones((len(xx), batch_max_len))
        batch_labels = -1 * np.zeros((len(xx), batch_max_len))
        
        j = 0
        while j < len(xx):
            cur_len = len(xx[j])
            batch_data[j][:cur_len] = xx[j]
            batch_labels[j][:cur_len] = yy[j]
            j += 1
        
        batch_data = torch.LongTensor(batch_data)
        batch_labels = torch.LongTensor(batch_labels)
        batch_data = Variable(batch_data)
        batch_labels = Variable(batch_labels)
        
        return batch_data, batch_labels, x_len, y_len

class CustomTestCollator(object):

    def __init__(self, vocab, label):
        self.params = vocab
        self.label = label

    def __call__(self, batch):
        xx = batch
        x_len = [len(x) for x in xx]
        # y_len = [len(y) for y in yy]
        batch_max_len = max([len(s) for s in xx])
        batch_data = self.params['<PAD>']*np.ones((len(xx), batch_max_len))
        # batch_labels = -1*np.zeros((len(xx), batch_max_len))
        for j in range(len(xx)):
            cur_len = len(xx[j])
            batch_data[j][:cur_len] = xx[j]
            # batch_labels[j][:cur_len] = yy[j]

        batch_data = torch.LongTensor(batch_data)
        batch_data = Variable(batch_data)

        return batch_data, x_len



def vocabulary_prep(dataset):
    vocab_set = set()
    i = 0
    # Iterate over each sentence in the dataset
    while i < len(dataset):
        # Initialize index variable
        j = 0
        # Iterate over each word in the sentence
        while j < len(dataset[i]):
            # Add the word to the set of unique words
            vocab_set.add(dataset[i][j])
            j += 1
        i += 1
    # Return the set of unique words
    return vocab_set

def index_prep(train_x, val_x, test_x):
    word_idx = {"<PAD>": 0, "<UNK>": 1}
    all_data = train_x + val_x + test_x
    i = 0
    while i < len(all_data):
        j = 0
        while j < len(all_data[i]):
            word = all_data[i][j]
            if word not in word_idx:
                word_idx[word] = len(word_idx)
            j += 1
        i += 1
    return word_idx

def embedding_matrix(word_idx, emb_dict, dimension):
    vocab_size = len(word_idx)
    emb_matrix = np.zeros((vocab_size, dimension))
    unk_embedding = emb_dict.get("<UNK>")
    i = 0
    while i < vocab_size:
        word = list(word_idx.keys())[i]
        idx = word_idx[word]
        emb = emb_dict.get(word, emb_dict.get(word.lower(), unk_embedding))
        emb_matrix[idx] = emb
        i += 1
    return emb_matrix

def sentences_vectors(train_x, word_idx):
    # Initialize an empty list to store the vectorized sentences
    train_x_vec = []
    # Iterate over each sentence in the list of sentences
    for sent in train_x:
        # Initialize an empty list to store the vectorized words in the sentence
        sent_vec = []
        # Iterate over each word in the sentence
        for word in sent:
            # Get the index of the word from the dictionary and append it to the list of vectorized words
            sent_vec.append(word_idx[word])
        # Append the list of vectorized words to the list of vectorized sentences
        train_x_vec.append(sent_vec)
    # Return the list of vectorized sentences
    return train_x_vec

def labels_dictionary(train_y, val_y):
    label = vocabulary_prep(train_y).union(vocabulary_prep(val_y))
    label_dict = {tag: i for i, tag in enumerate(label)}
    return label_dict

def labels_vectors(train_y, label_dict):
    train_y_vec = []
    i = 0
    while i < len(train_y):
        tags_vec = []
        j = 0
        while j < len(train_y[i]):
            label = train_y[i][j]
            if label in label_dict:
                tags_vec.append(label_dict[label])
            else:
                tags_vec.append(label_dict["<UNK>"])
            j += 1
        train_y_vec.append(tags_vec)
        i += 1
    return train_y_vec

# Create an index of all words in the dataset
index_wrds = index_prep(X_train, X_val, X_test)

# Convert text data into numerical vectors
X_train_vec = sentences_vectors(X_train, index_wrds)
X_test_vec = sentences_vectors(X_test, index_wrds)
X_val_vec = sentences_vectors(X_val, index_wrds)

# Create a dictionary of labels and convert label data into numerical vectors
label_dict = labels_dictionary(Y_train, Y_val)
Y_train_vec = labels_vectors(Y_train, label_dict)
Y_val_vec = labels_vectors(Y_val, label_dict)

def class_weights(label_dict, train_y, val_y):
    class_weights = {key: 0 for key in label_dict}
    total_nm_tags = 0
    for data in [train_y, val_y]:
        for tags in data:
            for tag in tags:
                total_nm_tags += 1
                class_weights[tag] += 1

    class_wt = [max(1.0, round(math.log(0.35* total_nm_tags / class_weights[key]), 2)) for key in class_weights]
    return torch.tensor(class_wt)


class_weight = class_weights(label_dict, Y_train, Y_val)
print(class_weight)

# Define BiLSTM model
model = BiLSTM(vocab_size=len(index_wrds), embedding_dim=100, linear_out_dim=128, 
               hidden_dim=256, lstm_layers=1, bidirectional=True, dropout_val=0.33, 
               tag_size=len(label_dict))
model.to(device)

# Define DataLoader and collator
# train_dataset = BiLSTM_DataLoader(X_train_vec, Y_train_vec)
# collator = CustomCollator(index_wrds, label_dict)
# dataloader = DataLoader(dataset=train_dataset, batch_size=8, drop_last=True, collate_fn=collator)

# # Define loss function, optimizer, and number of epochs
# class_weight = class_weights(label_dict, Y_train, Y_val)
# class_weight_tensor = torch.FloatTensor(class_weight).to(device)
# criterion = nn.CrossEntropyLoss(weight=class_weight_tensor)
# optimizer = torch.optim.SGD(model.parameters(), lr=0.12, momentum=0.9)

# num_epochs = 200

# # Train the BiLSTM model
# for epoch in range(num_epochs):
#     train_loss = 0.0
#     for input, label, input_len, label_len in dataloader:
#         optimizer.zero_grad()
#         output = model(input.to(device), input_len)
#         output = output.view(-1, len(label_dict))
#         label = label.view(-1)
#         loss = criterion(output, label.to(device))
#         loss.backward()
#         optimizer.step()
#         train_loss += loss.item() * input.size(1)
#     train_loss /= len(train_dataset)
#     print(f'Epoch: {epoch+1} \tTraining Loss: {train_loss:.6f}')
#     torch.save(model.state_dict(), f'drive/My Drive/CSCI544/hw4/model_BILSTM/model_bilstm_epoch_{epoch+1}.pt')


model=model.load_state_dict(torch.load("blstm1.pt"))

# Create a dataloader for the validation data
dataloader_dev = DataLoader(
    dataset=BiLSTM_DataLoader(X_val_vec, Y_val_vec),
    batch_size=1,
    shuffle=False,
    drop_last=True,
    collate_fn=CustomCollator(index_wrds, label_dict)
)

# Create reverse dictionaries for the labels and vocabulary
rev_label_dict = {v: k for k, v in label_dict.items()}
rev_vocab_dict = {v: k for k, v in index_wrds.items()}

# Open a file to write the output to
with open("drive/My Drive/CSCI544/hw4/dev1.out", 'w') as file:
    # Loop over the validation data
    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:
        # Get predictions from the model and convert to numpy arrays
        pred = model(dev_data.to(device), dev_data_len)
        pred = pred.cpu().detach().numpy()
        label = label.detach().numpy()
        dev_data = dev_data.detach().numpy()

        # Convert predictions to labels using the reverse label dictionary
        pred = np.argmax(pred, axis=2)
        pred = pred.reshape((len(label), -1))

        # Loop over the words in each sentence
        for i in range(len(dev_data)):
            for j in range(len(dev_data[i])):
                # Ignore padding tokens
                if dev_data[i][j] != 0:
                    # Get the word, gold label, and predicted label
                    word = rev_vocab_dict[dev_data[i][j]]
                    gold = rev_label_dict[label[i][j]]
                    op = rev_label_dict[pred[i][j]]

                    # Write the output to the file
                    file.write(" ".join([str(j+1), word, gold, op]))
                    file.write("\n")

            # Add a newline to separate sentences in the output
            file.write("\n")

# !perl 'drive/My Drive/CSCI544/hw4/conll03eval.txt' < 'drive/My Drive/CSCI544/hw4/dev1_train.out'

#Testing on Testing Dataset 
rev_label_dict = {v: k for k, v in label_dict.items()}
rev_vocab_dict = {v: k for k, v in index_wrds.items()}

BiLSTM_test = BiLSTM_TestLoader(X_test_vec)
custom_test_collator = CustomTestCollator(index_wrds, label_dict)
dataloader_test = DataLoader(dataset=BiLSTM_test,
                                batch_size=1,
                                shuffle=False,
                                drop_last=True,
                                collate_fn=custom_test_collator)


file = open("drive/My Drive/CSCI544/hw4/test1.out", 'w')
for test_data, test_data_len in dataloader_test:

    pred = model(test_data.to(device), test_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    test_data = test_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(test_data), -1))
    
    for i in range(len(test_data)):
        for j in range(len(test_data[i])):
            if test_data[i][j] != 0:
                word = rev_vocab_dict[test_data[i][j]]
                op = rev_label_dict[pred[i][j]]
                file.write(" ".join([str(j+1), word, op]))
                file.write("\n")

        file.write("\n")
        
file.close()





"""**Task 2: Using GloVe word embeddings**"""

glove = pd.read_csv('drive/My Drive/CSCI544/hw4/data/glove.6B.100d.txt', sep=" ", quoting=3, header=None, index_col=0)
glove_dict = {word: vector for word, vector in glove.T.items()}

word_to_index = index_prep(X_train, X_val, X_test)
glove_vectors = np.array([glove_dict[word] for word in glove_dict])
glove_dict["<PAD>"] = np.zeros((100,), dtype="float64")
glove_dict["<UNK>"] = np.mean(glove_vectors, axis=0, keepdims=True).reshape(100,)
embedding_matrix = embedding_matrix(word_idx=word_to_index, emb_dict=glove_dict, dimension=100)

vocab_size = embedding_matrix.shape[0]
embedding_size = embedding_matrix.shape[1]
print(vocab_size, embedding_size)

class BiLSTM_glove(nn.Module):
    def __init__(self, vocab_size, embedding_dim, linear_out_dim, hidden_dim, lstm_layers,
                 bidirectional, dropout_val, tag_size, emb_matrix):
        super(BiLSTM_glove, self).__init__()
        self.hidden_dim = hidden_dim
        self.lstm_layers = lstm_layers
        self.embedding_dim = embedding_dim
        self.linear_out_dim = linear_out_dim
        self.tag_size = tag_size
        self.emb_matrix = emb_matrix
        self.num_directions = 2 if bidirectional else 1

        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.embedding.weight = nn.Parameter(torch.tensor(emb_matrix))
        self.LSTM = nn.LSTM(embedding_dim, hidden_dim, num_layers=lstm_layers,
                            batch_first=True, bidirectional=bidirectional)
        self.fc = nn.Linear(hidden_dim*self.num_directions, linear_out_dim)
        self.dropout = nn.Dropout(dropout_val)
        self.elu = nn.ELU(alpha=0.01)
        self.classifier = nn.Linear(linear_out_dim, tag_size)

    def forward(self, sen, sen_len):
        batch_size = sen.shape[0]
        h_0, c_0 = (torch.zeros(self.lstm_layers * self.num_directions,
                                 batch_size, self.hidden_dim).to(device),
                    torch.zeros(self.lstm_layers * self.num_directions,
                                 batch_size, self.hidden_dim).to(device))

        embedded = self.embedding(sen).float()
        packed_embedded = pack_padded_sequence(embedded, sen_len, batch_first=True, enforce_sorted=False)
        output, _ = self.LSTM(packed_embedded, (h_0, c_0))
        output_unpacked, _ = pad_packed_sequence(output, batch_first=True)
        dropout = self.dropout(output_unpacked)
        lin = self.fc(dropout)
        pred = self.elu(lin)
        pred = self.classifier(pred)
        return pred

BiLSTM_model = BiLSTM_glove(
    vocab_size=len(index_wrds),
    embedding_dim=100,
    linear_out_dim=128,
    hidden_dim=256,
    lstm_layers=1,
    bidirectional=True,
    dropout_val=0.33,
    tag_size=len(label_dict),
    emb_matrix=embedding_matrix
).to(device)

print(BiLSTM_model)

# 

BiLSTM_model=BiLSTM_model.load_state_dict(torch.load("blstm2.pt"))

# Create a DataLoader for the validation dataset
dataloader_dev = DataLoader(
    dataset=BiLSTM_DataLoader(X_val_vec, Y_val_vec),
    batch_size=8,
    shuffle=False,
    drop_last=True,
    collate_fn=CustomCollator(index_wrds, label_dict)
)

# Create dictionaries for mapping label indices to labels and word indices to words
rev_label_dict = {v: k for k, v in label_dict.items()}
rev_vocab_dict = {v: k for k, v in index_wrds.items()}
res = [] 
# Open a file for writing the output
with open("drive/My Drive/CSCI544/hw4/dev2.out", 'w') as file:
    # Iterate over batches of data in the validation DataLoader
    for dev_data, label, dev_data_len, label_data_len in dataloader_dev:
        # Run the BiLSTM model on the batch and convert the output to a NumPy array
        pred = BiLSTM_model(dev_data.to(device), dev_data_len).cpu().detach().numpy()
        # Convert the label tensor to a NumPy array
        label = label.detach().numpy()
        # Convert the input tensor to a NumPy array
        dev_data = dev_data.detach().numpy()
        # Compute the predicted labels for each word in the batch
        pred = np.argmax(pred, axis=2).reshape((len(label), -1))
        # Iterate over words in the batch
        for i in range(len(dev_data)):
            for j in range(len(dev_data[i])):
                # If the word is not a padding token, look up its label and predicted label
                if dev_data[i][j] != 0:
                    word = rev_vocab_dict[dev_data[i][j]]
                    gold = rev_label_dict[label[i][j]]
                    op = rev_label_dict[pred[i][j]]
                    # Append the word and its labels to the output list and write to the output file
                    res.append((word, gold, op))
                    file.write(" ".join([str(j + 1), word, gold, op]))
                    file.write("\n")
            file.write("\n")

# !perl 'drive/My Drive/CSCI544/hw4/conll03eval.txt' < 'drive/My Drive/CSCI544/hw4/dev2_train.out'

#predicting for testing dataset

BiLSTM_test = BiLSTM_TestLoader(X_test_vec)
custom_test_collator = CustomTestCollator(index_wrds, label_dict)
dataloader_test = DataLoader(dataset=BiLSTM_test,
                                batch_size=1,
                                shuffle=False,
                                drop_last=True,
                                collate_fn=custom_test_collator)

rev_label_dict = {v: k for k, v in label_dict.items()}
rev_vocab_dict = {v: k for k, v in index_wrds.items()}
res = []
file = open("drive/My Drive/CSCI544/hw4/test2.out", 'w')
for test_data, test_data_len in dataloader_test:

    pred = BiLSTM_model(test_data.to(device), test_data_len)
    pred = pred.cpu()
    pred = pred.detach().numpy()
    # label = label.detach().numpy()
    test_data = test_data.detach().numpy()
    pred = np.argmax(pred, axis=2)
    pred = pred.reshape((len(test_data), -1))

    for i in range(len(test_data)):
        for j in range(len(test_data[i])):
            if test_data[i][j] != 0:
                word = rev_vocab_dict[test_data[i][j]]
                # gold = rev_label_dict[label[i][j]]
                op = rev_label_dict[pred[i][j]]
                res.append((word, op))
                file.write(" ".join([str(j + 1), word, op]))
                file.write("\n")
        file.write("\n")
file.close()

