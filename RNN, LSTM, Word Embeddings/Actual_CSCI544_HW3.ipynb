{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2401,
     "status": "ok",
     "timestamp": 1677654867023,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "cPo5ic-dnhoV",
    "outputId": "5af91c99-980d-480a-f717-07b5ccdd1387"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4288,
     "status": "ok",
     "timestamp": 1677654871298,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "EeaxxEgGn12m",
    "outputId": "3f851008-c944-459e-bd52-b5f84741176d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.1.1)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.13.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 8603,
     "status": "ok",
     "timestamp": 1677654879881,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "0AJhKwKOnmGV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import contractions\n",
    "import re\n",
    "import math\n",
    "import gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1677654880216,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "f24tL51Rn1Yy",
    "outputId": "4350bc4b-5c88-40bf-a22b-2d276b7f712b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 313,
     "status": "ok",
     "timestamp": 1677654880512,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "zhiWUe23n07k"
   },
   "outputs": [],
   "source": [
    "#df=pd.read_table('drive/My Drive/CSCI544/HW3/HW 3/amazon_reviews_us_Beauty_v1_00.tsv',on_bad_lines='skip',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677654880513,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "q_GD1_Zgnzwk"
   },
   "outputs": [],
   "source": [
    "#df_fnl=df[['review_body','star_rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677654880514,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "ypjWxc-joX_K"
   },
   "outputs": [],
   "source": [
    "#df_final=df_fnl.replace({'star_rating':{2:1,3:2,4:3,5:3,'5':3,'2':1,'3':2,'4':3,'1':1}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677654880515,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "xxjBdkomoaoP"
   },
   "outputs": [],
   "source": [
    "#df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677654880515,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "fFkb5akhocku"
   },
   "outputs": [],
   "source": [
    "#df_final=df_final.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677654880516,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "Zzq2Il0togmU"
   },
   "outputs": [],
   "source": [
    "#df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677654880517,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "z8XVLQ74okT8"
   },
   "outputs": [],
   "source": [
    "# s0 = df_final[df_final['star_rating'].eq(1)].sample(20000).index\n",
    "# s1 = df_final[df_final['star_rating'].eq(2)].sample(20000).index \n",
    "# s2 = df_final[df_final['star_rating'].eq(3)].sample(20000).index \n",
    "\n",
    "# df_fi = df_final.loc[s0.union(s1).union(s2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1677654880522,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "FJ1mhTTRooJK"
   },
   "outputs": [],
   "source": [
    "#def remove_alphanumeric(s):\n",
    "#     s=s.lower()\n",
    "#     s=s.strip()\n",
    "#     s=contractions.fix(s)\n",
    "#     s=s.replace(r'<[^<>]*>', '')\n",
    "#     s=s.replace(r'http\\S+', '').replace(r'www\\S+', '')\n",
    "#     s=re.sub(r'[^a-zA-Z]',' ',s)\n",
    "#     return re.sub(' +', ' ',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1677654880523,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "FN8b-nojou3p"
   },
   "outputs": [],
   "source": [
    "#df_fi['review_body']=df_fi['review_body'].apply(remove_alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1677654880524,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "s0HOK5CGdBOy"
   },
   "outputs": [],
   "source": [
    "# path='drive/My Drive/CSCI544/HW3/HW 3/main_dataframe.csv'\n",
    "\n",
    "# with open(path,'w',encoding='utf-8-sig') as f:\n",
    "#   df_fi.to_csv(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1120,
     "status": "ok",
     "timestamp": 1677654881627,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "-HJ_gaOgeY94"
   },
   "outputs": [],
   "source": [
    "#df_fi=pd.read_csv('drive/My Drive/CSCI544/HW3/HW 3/main_dataframe.csv')\n",
    "\n",
    "df_fi=pd.read_csv('main_dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EFvLo5wo4DY"
   },
   "source": [
    "**Word Embeddings**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWlS0E_cpDbK"
   },
   "source": [
    "Part a Pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_wrd2vc = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 35759,
     "status": "ok",
     "timestamp": 1677654917380,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "WgNW5rnioxWg"
   },
   "outputs": [],
   "source": [
    "# from gensim.models import KeyedVectors\n",
    "# google_wrd2vc= KeyedVectors.load('google_wrd2vc.kv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1677654917381,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "daBEQHjso9xi",
    "outputId": "d19c493d-c71b-4298-db88-0f2cf93c1fc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61207294"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_wrd2vc.similarity('good','better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1677654917382,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "OVVZMBGXo_l2",
    "outputId": "05b6dd7a-5058-4698-c1b0-d34d57b0d5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16873193"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_wrd2vc.similarity('machine','sounds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eTtbDX0npJ_K"
   },
   "source": [
    "Part B Own corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1677654917383,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "Zk5nUidqpGh0"
   },
   "outputs": [],
   "source": [
    "# class MyCorpus:\n",
    "#     ## step to create custom Corpus for Word2vec to use to generate word vectors\n",
    "#     def __iter__(self):\n",
    "#         for line in df_fi['review_body']:\n",
    "#             # assume there's one document per line, tokens separated by whitespace\n",
    "#             yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1677654917384,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "crVJ8dHRpCFm"
   },
   "outputs": [],
   "source": [
    "# #Loading my own Word2Vec \n",
    "# my_model= Word2Vec(sentences = MyCorpus(), size = 300, window = 13, min_count = 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1677654917385,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "YbdSZA14pRQ0"
   },
   "outputs": [],
   "source": [
    "# my_model.wv.similarity('machine','sounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1677654917386,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "yhqPxwfNpS6P"
   },
   "outputs": [],
   "source": [
    "# my_model.wv.similarity('good','better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1677654917387,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "BVI-lfK8pUYR"
   },
   "outputs": [],
   "source": [
    "# print(my_model.wv.most_similar(positive=['king', 'woman'], negative=['man'],topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rjANHcYqpzgI"
   },
   "source": [
    "The performance of the Pre-trained model by word2vec-google-news-300 is better since it has a much bigger corpus than my personal dataset. Therefore the pretrained mmodel can findout better similarities than my model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4rGgpNy-p1S5"
   },
   "source": [
    "**Simple Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1677654917388,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "qBJVUpa9pWZR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917388,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "C_NWZjLZsTVk"
   },
   "outputs": [],
   "source": [
    "# def wrd2vc_avg(strings, wv_model):\n",
    "#   sent_split = strings.split(' ') if isinstance(strings, str) else strings\n",
    "#   sum = np.zeros(shape=(300,))\n",
    "#   count = 0\n",
    "#   for word in sent_split:\n",
    "#     if word in wv_model:\n",
    "#       wv = wv_model[word]\n",
    "#       sum += wv\n",
    "#       count += 1\n",
    "#   avg = sum / count\n",
    "#   return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917389,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "0Y5Brct-u8rJ"
   },
   "outputs": [],
   "source": [
    "# def idx_nan(matrix):\n",
    "#     idx = np.isnan(matrix).any(axis=1).nonzero()[0]\n",
    "#     if len(idx) > 0:\n",
    "#         return idx[::300]\n",
    "#     else:\n",
    "#         return None\n",
    "\n",
    "def nan_id(mtrx):\n",
    "    \n",
    "      if np.any(np.isnan(mtrx)):\n",
    "        nanarray=np.argwhere(np.isnan(mtrx))\n",
    "        nannum=nanarray.shape[0]\n",
    "        array=np.arange(0,nannum,300)\n",
    "\n",
    "        id=[]\n",
    "        for x in array:\n",
    "            \n",
    "            id.append(nanarray[x][0])\n",
    "        return id\n",
    "      else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1677654917390,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "qdCghtW5vgaT"
   },
   "outputs": [],
   "source": [
    "# true_labels= df_fi['star_rating'].to_numpy()\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(df_fi['review_body'], true_labels, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917392,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "7brByIF2xpKW"
   },
   "outputs": [],
   "source": [
    "# X_train_temp = X_train.apply(lambda x: wrd2vc_avg(x, google_wrd2vc))\n",
    "# X_train_fnl = np.array(X_train_temp.values.tolist())\n",
    "\n",
    "# X_test_temp= X_test.apply(lambda x: wrd2vc_avg(x, google_wrd2vc))\n",
    "# X_test_fnl = np.array(X_test_temp.values.tolist())\n",
    "\n",
    "# # Remove the NaN's (if any) and their labels from the training and test datasets \n",
    "# id_nan_trn = nan_id(X_train_fnl)\n",
    "# if id_nan_trn == None:\n",
    "#   Y_train_fnl = Y_train\n",
    "# else:\n",
    "#   X_train_fnl = np.delete(X_train_fnl, id_nan_trn, 0)\n",
    "#   Y_train_fnl= np.delete(Y_train, id_nan_trn)\n",
    "\n",
    "# '''make another Y_train_fnl for fnn so that dont have to run the whole thing again and dont make that variable as int since its not used ...so that ram is saved'''\n",
    "# Y_trn_fnn_fnl=Y_train_fnl\n",
    "# Y_train_fnl=Y_train_fnl.astype('int')\n",
    "\n",
    "# id_nan_tst = nan_id(X_test_fnl)\n",
    "# if id_nan_tst == None:\n",
    "#   Y_test_fnl= Y_test\n",
    "# else:\n",
    "#   X_test_fnl = np.delete(X_test_fnl, id_nan_tst, 0)\n",
    "#   Y_test_fnl = np.delete(Y_test, id_nan_tst)\n",
    "# Y_tst_fnn_fnl=Y_test_fnl\n",
    "# Y_test_fnl=Y_test_fnl.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917393,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "PobiqQa8yML7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917394,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "chCDz9cBy0GM"
   },
   "outputs": [],
   "source": [
    "# clf=Perceptron()\n",
    "# clf.fit(X_train_fnl,Y_train_fnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1677654917395,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "Mdq7nWLRzuFq"
   },
   "outputs": [],
   "source": [
    "# predicted_perceptron=clf.predict(X_test_fnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1677654917396,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "AvMNd2hv1u3D"
   },
   "outputs": [],
   "source": [
    "#print(f'accuracry score:{accuracy_score(Y_test_fnl,predicted_perceptron)*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5-9k1OX1-62"
   },
   "source": [
    "SVM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1677654917397,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "s9lYMHhC17eI"
   },
   "outputs": [],
   "source": [
    "# clf_SVM=LinearSVC()\n",
    "# clf_SVM.fit(X_train_fnl,Y_train_fnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917397,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "IKM7jGzc2HGu"
   },
   "outputs": [],
   "source": [
    "#predicted_SVM=clf_SVM.predict(X_test_fnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917398,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "RVeqqnjq2efk"
   },
   "outputs": [],
   "source": [
    "#print(f'accruacy score for SVM is:{accuracy_score(Y_test_fnl,predicted_SVM)*100} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJc4ySVl2pWj"
   },
   "source": [
    "**FNN** \n",
    "Part 4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917399,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "9xmY8DPw2nu8"
   },
   "outputs": [],
   "source": [
    "# true_labels= df_fi['star_rating'].to_numpy()\n",
    "# X_trn_fnn, X_test_fnn, Y_trn_fnn, Y_test_fnn = train_test_split(df_fi['review_body'], true_labels, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 123,
     "status": "ok",
     "timestamp": 1677654917400,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "po9iedhyT7KC"
   },
   "outputs": [],
   "source": [
    "#X_trn_fnn_fnl,Y_trn_fnn_fnl,X_tst_fnn_fnl,Y_tst_fnn_fnl=X_train_fnl,Y_trn_fnn_fnl,X_test_fnl,Y_test_fnl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917400,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "597FYP5aUZyS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917401,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "u92xsOgb70FZ"
   },
   "outputs": [],
   "source": [
    "#len(Y_trn_fnn_fnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1677654917402,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "OgvKpjaa3OGy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1677654917403,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "v-tbHzfA7oxT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1677654917403,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "m6AwkQ374Lh3"
   },
   "outputs": [],
   "source": [
    "class Train(Dataset):\n",
    "    \n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        'Initialization'\n",
    "        self.data = xtrain\n",
    "        self.labels = ytrain\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1677654917404,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "jTpYK5um4P4G"
   },
   "outputs": [],
   "source": [
    "class Test(Dataset):\n",
    "    def __init__(self, xtest, ytest):\n",
    "        'Initialization'\n",
    "        self.data = xtest\n",
    "        self.labels = ytest\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1677654917405,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "cGLHeglu4RyW"
   },
   "outputs": [],
   "source": [
    "# train_data_fnn, test_data_fnn = Train(X_trn_fnn_fnl, Y_trn_fnn_fnl-1), Test(X_tst_fnn_fnl, Y_tst_fnn_fnl-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1677654917405,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "FFQMiCn_4k8n"
   },
   "outputs": [],
   "source": [
    "# # number of subprocesses to use for data loading\n",
    "# num_workers = 0\n",
    "# # how many samples per batch to load\n",
    "# batch_size = 100\n",
    "# # percentage of training set to use as validation\n",
    "# valid_size = 0.2\n",
    "\n",
    "# # obtain training indices that will be used for validation\n",
    "# num_train = len(train_data_fnn)\n",
    "# indices = list(range(num_train))\n",
    "# np.random.shuffle(indices)\n",
    "# split = int(np.floor(valid_size * num_train))\n",
    "# train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# # define samples for obtaining training and validation batches\n",
    "# train_sampler = SubsetRandomSampler(train_idx)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# # prepare data loaders\n",
    "# train_loader_fnn = torch.utils.data.DataLoader(train_data_fnn, batch_size=batch_size,\n",
    "#                                            sampler=train_sampler)\n",
    "# valid_loader_fnn = torch.utils.data.DataLoader(train_data_fnn, batch_size=batch_size,\n",
    "#                                            sampler=valid_sampler)\n",
    "# test_loader_fnn = torch.utils.data.DataLoader(test_data_fnn, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1677654917406,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "DHPlkocK4utW"
   },
   "outputs": [],
   "source": [
    "# # define the MLP architecture\n",
    "# class ThreeLayerMLP(nn.Module):\n",
    "#   def __init__(self, D_in, H1, H2, D_out):\n",
    "#     super().__init__()\n",
    "#     self.linear1 = nn.Linear(D_in, H1)\n",
    "#     self.linear2 = nn.Linear(H1, H2)\n",
    "#     self.linear3 = nn.Linear(H2, D_out)\n",
    "#     # dropout layer (p=0.2)\n",
    "#     self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "#   def forward(self, x):\n",
    "#     # add hidden layer, with relu activation function\n",
    "#     h1_relu = F.relu(self.linear1(x))\n",
    "#     # add dropout layer\n",
    "#     # add another hidden layer, with relu activation function\n",
    "#     h2_relu = F.relu(self.linear2(h1_relu))\n",
    "#     # add another dropout layer\n",
    "#     h2_drop = self.dropout(h2_relu)\n",
    "#     # add output layer\n",
    "#     h2_output = self.linear3(h2_drop)\n",
    "\n",
    "#     return h2_output\n",
    "\n",
    "# # initialize MLP\n",
    "# model_fnn = ThreeLayerMLP(300, 100, 10, 4)\n",
    "# #model_fnn.cuda()\n",
    "# print(model_fnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1677654917407,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "Ldn3_JUv43n0"
   },
   "outputs": [],
   "source": [
    "# # specify loss function (categorical cross-entropy)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "# optimizer_fnn = torch.optim.SGD(model_fnn.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "executionInfo": {
     "elapsed": 121,
     "status": "ok",
     "timestamp": 1677654917408,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "8-bX-Q_g5K2-"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# # number of epochs to train the model\n",
    "# n_epochs = 50\n",
    "\n",
    "# # initialize tracker for minimum validation loss\n",
    "# valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#   # monitor training loss\n",
    "#   train_loss = 0.0\n",
    "#   valid_loss = 0.0\n",
    "\n",
    "#   ###################\n",
    "#   # train the model #\n",
    "#   ###################\n",
    "#   model_fnn.train() # prep model for training\n",
    "#   for data, target in train_loader_fnn:\n",
    "#     # transfer data and target to GPU\n",
    "#     data, target = data.to(device), target.to(device)\n",
    "#     # clear the gradients of all optimized variables\n",
    "#     optimizer_fnn.zero_grad()\n",
    "#     # forward pass: compute predicted outputs by passing inputs to the model\n",
    "#     output = model_fnn(data.float())\n",
    "#     # calculate the loss\n",
    "#     loss = criterion(output, target)\n",
    "#     # backward pass: compute gradient of the loss with respect to model parameters\n",
    "#     loss.backward()\n",
    "#     # perform a single optimization step (parameter update)\n",
    "#     optimizer_fnn.step()\n",
    "#     # update running training loss\n",
    "#     train_loss += loss.item()*data.size(0)\n",
    "\n",
    "#   ######################\n",
    "#   # validate the model #\n",
    "#   ######################\n",
    "#   model_fnn.eval() # prep model for evaluation\n",
    "#   for data, target in valid_loader_fnn:\n",
    "#     # transfer data and target to GPU\n",
    "#     data, target = data.to(device), target.to(device)\n",
    "#     # forward pass: compute predicted outputs by passing inputs to the model\n",
    "#     output = model_fnn(data.float())\n",
    "#     # calculate the loss\n",
    "#     loss = criterion(output, target)\n",
    "#     # update running validation loss\n",
    "#     valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "#   # print training/validation statistics\n",
    "#   # calculate average loss over an epoch\n",
    "#   train_loss = train_loss/len(train_loader_fnn.dataset)\n",
    "#   valid_loss = valid_loss/len(valid_loader_fnn.dataset)\n",
    "\n",
    "#   print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "#         epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "#   # save model if validation loss has decreased\n",
    "#   if valid_loss <= valid_loss_min:\n",
    "#     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "#         .format(valid_loss_min, valid_loss))\n",
    "#     torch.save(model_fnn.state_dict(), 'model.pt')\n",
    "#     valid_loss_min = valid_loss\n",
    "\n",
    "# end = time.time()\n",
    "# print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1677654917409,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "zg8x-hPx52yN"
   },
   "outputs": [],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "#model_fnn.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1677654917409,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "hy1Ae1s6-fDM"
   },
   "outputs": [],
   "source": [
    "# correct = 0\n",
    "# total = 0\n",
    "# model_fnn.eval()  # set the model to evaluation mode\n",
    "# with torch.no_grad():\n",
    "#     for embeddings, labels in test_loader_fnn:\n",
    "#         # move data to the device\n",
    "#         embeddings = embeddings.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         # calculate outputs by running embeddings through the network\n",
    "#         outputs = model_fnn(embeddings.float())\n",
    "#         # the class with the highest score is what we choose as prediction\n",
    "#         _, predicted = torch.max(outputs, dim=1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Accuracy_MLP-model_average-word-vectors_pretrained-word2vec-model_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGCo67eh_oIj"
   },
   "source": [
    "Part 4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1677654917410,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "wfHtH6UZ_A2W"
   },
   "outputs": [],
   "source": [
    "## A function get the first 10 word vector embeddings and if length of reviews is less than 10 ..padding the remaining part to get to 10 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1677654917411,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "ZgyERnc_AEgu"
   },
   "outputs": [],
   "source": [
    "# def concat_10_pad_10(sentence, model):\n",
    "#     if type(sentence) == str:\n",
    "#         sentence = sentence.split()\n",
    "\n",
    "#     padded_vec = np.zeros((10, 300))\n",
    "\n",
    "#     for i, word in enumerate(sentence[:10]):\n",
    "#         if word in model:\n",
    "#             padded_vec[i] = model[word]\n",
    "\n",
    "#     return padded_vec.reshape(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1677654917412,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "c1HCf4AYCbwu"
   },
   "outputs": [],
   "source": [
    "# true_labels= df_fi['star_rating'].to_numpy()\n",
    "# X_trn_4b, X_test_4b, Y_trn_4b, Y_test_4b = train_test_split(df_fi['review_body'], true_labels, test_size=0.2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1677654917412,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "tRN7XnFFC3eW"
   },
   "outputs": [],
   "source": [
    "# X_trn_4b_temp= X_trn_4b.apply(lambda x: concat_10_pad_10(x, google_wrd2vc))\n",
    "# X_trn_4b_fnl = np.array(X_trn_4b_temp.values.tolist())\n",
    "\n",
    "# X_tst_4b_temp= X_test_4b.apply(lambda x: concat_10_pad_10(x, google_wrd2vc))\n",
    "# X_tst_4b_fnl = np.array(X_tst_4b_temp.values.tolist())\n",
    "\n",
    "# # Remove the NaN's (if any) and their labels from the training and test datasets \n",
    "# id_nan_trn = nan_id(X_trn_4b_fnl)\n",
    "# if id_nan_trn != None:\n",
    "#   X_trn_4b_fnl = np.delete(X_trn_4b_fnl, id_nan_trn, 0)\n",
    "#   Y_trn_4b_fnl= np.delete(Y_trn_4b, id_nan_trn)\n",
    "# else:\n",
    "#     Y_trn_4b_fnl = Y_trn_4b\n",
    "# #Y_train_fnl=Y_train_fnl.astype('int')\n",
    "\n",
    "# id_nan_tst = nan_id(X_tst_4b_fnl)\n",
    "# if id_nan_tst != None:\n",
    "#   X_tst_4b_fnl = np.delete(X_tst_4b_fnl, id_nan_tst, 0)\n",
    "#   Y_tst_4b_fnl = np.delete(Y_test_4b, id_nan_tst)\n",
    "# else:\n",
    "#   Y_tst_4b_fnl=Y_test_4b\n",
    "# #Y_test_fnl=Y_test_fnl.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1677654917413,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "psKfTPlTDnM5"
   },
   "outputs": [],
   "source": [
    "#train_data_fnn_10, test_data_fnn_10 = Train(X_trn_4b_fnl, Y_trn_4b_fnl-1), Test(X_tst_4b_fnl, Y_tst_4b_fnl-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1677654917414,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "uCwx-cAxcOrh"
   },
   "outputs": [],
   "source": [
    "# # number of subprocesses to use for data loading\n",
    "# num_workers = 0\n",
    "# # how many samples per batch to load\n",
    "# batch_size = 100\n",
    "# # percentage of training set to use as validation\n",
    "# valid_size = 0.2\n",
    "\n",
    "# # obtain training indices that will be used for validation\n",
    "# num_train = len(train_data_fnn_10)\n",
    "# indices = list(range(num_train))\n",
    "# np.random.shuffle(indices)\n",
    "# split = int(np.floor(valid_size * num_train))\n",
    "# train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# # define samples for obtaining training and validation batches\n",
    "# train_sampler = SubsetRandomSampler(train_idx)\n",
    "# valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# # prepare data loaders\n",
    "# train_loader_fnn_10 = torch.utils.data.DataLoader(train_data_fnn_10, batch_size=batch_size,\n",
    "#                                            sampler=train_sampler)\n",
    "# valid_loader_fnn_10 = torch.utils.data.DataLoader(train_data_fnn_10, batch_size=batch_size,\n",
    "#                                            sampler=valid_sampler)\n",
    "# test_loader_fnn_10 = torch.utils.data.DataLoader(test_data_fnn_10, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "executionInfo": {
     "elapsed": 117,
     "status": "ok",
     "timestamp": 1677654917415,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "51_mol2RnEa1"
   },
   "outputs": [],
   "source": [
    "#model_fnn_10 = ThreeLayerMLP(3000, 100, 10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 116,
     "status": "ok",
     "timestamp": 1677654917415,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "RI5oCypakbKg"
   },
   "outputs": [],
   "source": [
    "# model_fnn_10 = ThreeLayerMLP(300, 100, 10, 4)\n",
    "#print(model_fnn_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1677654917416,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "jJ0RhhbpkmPF"
   },
   "outputs": [],
   "source": [
    "# # specify loss function (categorical cross-entropy)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# # specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "# optimizer_fnn_10 = torch.optim.SGD(model_fnn_10.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1677654917417,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "M4c6YUiokrOi"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "# # number of epochs to train the model\n",
    "# n_epochs = 20\n",
    "\n",
    "# # initialize tracker for minimum validation loss\n",
    "# valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "# for epoch in range(n_epochs):\n",
    "#   # monitor training loss\n",
    "#   train_loss = 0.0\n",
    "#   valid_loss = 0.0\n",
    "\n",
    "#   ###################\n",
    "#   # train the model #\n",
    "#   ###################\n",
    "#   model_fnn_10.train() # prep model for training\n",
    "#   for data, target in train_loader_fnn_10:\n",
    "#     # transfer data and target to GPU\n",
    "#     data, target = data.to(device), target.to(device)\n",
    "#     # clear the gradients of all optimized variables\n",
    "#     optimizer_fnn_10.zero_grad()\n",
    "#     # forward pass: compute predicted outputs by passing inputs to the model\n",
    "#     output = model_fnn_10(data.float())\n",
    "#     # calculate the loss\n",
    "#     loss = criterion(output, target)\n",
    "#     # backward pass: compute gradient of the loss with respect to model parameters\n",
    "#     loss.backward()\n",
    "#     # perform a single optimization step (parameter update)\n",
    "#     optimizer_fnn_10.step()\n",
    "#     # update running training loss\n",
    "#     train_loss += loss.item()*data.size(0)\n",
    "\n",
    "#   ######################\n",
    "#   # validate the model #\n",
    "#   ######################\n",
    "#   model_fnn_10.eval() # prep model for evaluation\n",
    "#   for data, target in valid_loader_fnn_10:\n",
    "#     # transfer data and target to GPU\n",
    "#     data, target = data.to(device), target.to(device)\n",
    "#     # forward pass: compute predicted outputs by passing inputs to the model\n",
    "#     output = model_fnn_10(data.float())\n",
    "#     # calculate the loss\n",
    "#     loss = criterion(output, target)\n",
    "#     # update running validation loss\n",
    "#     valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "#   # print training/validation statistics\n",
    "#   # calculate average loss over an epoch\n",
    "#   train_loss = train_loss/len(train_loader_fnn_10.dataset)\n",
    "#   valid_loss = valid_loss/len(valid_loader_fnn_10.dataset)\n",
    "\n",
    "#   print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "#         epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "#   # save model if validation loss has decreased\n",
    "#   if valid_loss <= valid_loss_min:\n",
    "#     print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "#         .format(valid_loss_min, valid_loss))\n",
    "#     torch.save(model_fnn_10.state_dict(), 'model.pt')\n",
    "#     valid_loss_min = valid_loss\n",
    "\n",
    "# end = time.time()\n",
    "# print('Time elapsed: %.2f s' % (end - start))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1677654917417,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "yXv6p_dalBWQ"
   },
   "outputs": [],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "#model_fnn_10.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1677654917418,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "_md1X7imr6xo"
   },
   "outputs": [],
   "source": [
    "# correct = 0\n",
    "# total = 0\n",
    "# model_fnn_10.eval()  # set the model to evaluation mode\n",
    "# with torch.no_grad():\n",
    "#     for embeddings, labels in test_loader_fnn_10:\n",
    "#         # move data to the device\n",
    "#         embeddings = embeddings.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         # calculate outputs by running embeddings through the network\n",
    "#         outputs = model_fnn_10(embeddings.float())\n",
    "#         # the class with the highest score is what we choose as prediction\n",
    "#         _, predicted = torch.max(outputs, dim=1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Accuracy_MLP-model_average-word-vectors_pretrained-word2vec-model_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N2epACDsHF4"
   },
   "source": [
    "**Part 5) Recurrent Neural Networks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1677654917418,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "pIZ6KdXLsB2K"
   },
   "outputs": [],
   "source": [
    "##Function to truncate longer reviews by 20 words and padding the smaller reviews to 20 to give input to the Functions of Recurrent Neural Network\n",
    "\n",
    "# def trunc_20_pad_20(strings,model):\n",
    "    \n",
    "#       sent_split = [strings.split(' ') if isinstance(strings, str) else strings]\n",
    "\n",
    "#        word_vector = []\n",
    "#       for i in range(20):\n",
    "            \n",
    "#               try:\n",
    "#                 wv = model[sent_split[i]]\n",
    "#                 word_vector.append(wv)\n",
    "#               except:\n",
    "                \n",
    "#                 pass\n",
    "\n",
    "#               if len(word_vector) < 20:\n",
    "#                 for _ in range(20-len(word_vector)):\n",
    "#                   word_vector.append([0 for _ in range(300)])\n",
    "\n",
    "#     return word_vector\n",
    "def trunc_20_pad_20(strings,model):\n",
    "    sent_split=[strings.split(' ') if isinstance (strings, str) else strings]\n",
    "    \n",
    "    wv=[]\n",
    "    for i in range(20):\n",
    "        try:\n",
    "            temp=model[sent_split[i]]\n",
    "            wv.append(temp)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    if len(wv)<20:\n",
    "        for pad in range(20-len(wv)):\n",
    "            wv.append([0 for pad in range(300)])\n",
    "    \n",
    "    return wv\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "executionInfo": {
     "elapsed": 112,
     "status": "ok",
     "timestamp": 1677654917419,
     "user": {
      "displayName": "Ankit Tripathi",
      "userId": "15853411522341197702"
     },
     "user_tz": 480
    },
    "id": "9gBY_y4VzT9U"
   },
   "outputs": [],
   "source": [
    "true_labels= df_fi['star_rating'].to_numpy()\n",
    "X_trn_RNN, X_test_RNN, Y_trn_RNN, Y_test_RNN = train_test_split(df_fi['review_body'], true_labels, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwXodEyS0iXP"
   },
   "outputs": [],
   "source": [
    "X_trn_rnn_temp= X_trn_RNN.apply(lambda x: trunc_20_pad_20(x, google_wrd2vc))\n",
    "X_trn_rnn_fnl = np.array(X_trn_rnn_temp.values.tolist())\n",
    "\n",
    "X_tst_rnn_temp= X_test_RNN.apply(lambda x: trunc_20_pad_20(x, google_wrd2vc))\n",
    "X_tst_rnn_fnl = np.array(X_tst_rnn_temp.values.tolist())\n",
    "\n",
    "# Remove the NaN's (if any) and their labels from the training and test datasets \n",
    "# id_nan_trn = nan_id(X_trn_rnn_fnl)\n",
    "# if id_nan_trn != None:\n",
    "    \n",
    "#     X_trn_rnn_fnl = np.delete(X_trn_rnn_fnl, id_nan_trn, 0)\n",
    "#     Y_trn_rnn_fnl= np.delete(Y_trn_RNN, id_nan_trn)\n",
    "# else:\n",
    "Y_trn_rnn_fnl = Y_trn_RNN\n",
    "# #Y_train_fnl=Y_train_fnl.astype('int')\n",
    "\n",
    "# id_nan_tst = nan_id(X_tst_rnn_fnl)\n",
    "# if id_nan_tst != None:\n",
    "    \n",
    "#     X_tst_rnn_fnl = np.delete(X_tst_rnn_fnl, id_nan_tst, 0)\n",
    "#     Y_tst_rnn_fnl = np.delete(Y_test_RNN, id_nan_tst)\n",
    "# else:\n",
    "Y_tst_rnn_fnl=Y_test_RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ZBUb09nO1uAs"
   },
   "outputs": [],
   "source": [
    "#Creating the training and testing dataset\n",
    "\n",
    "train_data_rnn= Train(X_trn_rnn_fnl, Y_trn_rnn_fnl- 1)\n",
    "test_data_rnn = Test(X_tst_rnn_fnl, Y_tst_rnn_fnl - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "sh4DSxyp331_"
   },
   "outputs": [],
   "source": [
    "#Batching and Loading data for the RNN Model\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 16\n",
    "valid_size = 0.2\n",
    "\n",
    "num_train = len(train_data_rnn)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader_rnn = torch.utils.data.DataLoader(train_data_rnn, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_rnn = torch.utils.data.DataLoader(train_data_rnn, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_rnn = torch.utils.data.DataLoader(test_data_rnn, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "4CFDBlM-5DWA"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "      def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super().__init__()\n",
    "        # Number of hidden dimensions\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of hidden layers\n",
    "        self.layer_dim = layer_dim\n",
    "        # RNN\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True,\n",
    "                         nonlinearity='relu')\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "      def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out=out.contiguous().view(-1,out.shape[1]*out.shape[2])\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wcJ123EW7nuf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_rnn_nrml=RNN(300,20,1,4)\n",
    "#model_rnn_nrml.cuda()\n",
    "print(model_rnn_nrml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RmfwrZWv8A_f"
   },
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "\n",
    "optimizer_rnn = torch.optim.SGD(model_rnn_nrml.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WtNWPllD8SwS"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [31]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# train the model #\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m###################\u001b[39;00m\n\u001b[0;32m     18\u001b[0m model_rnn_nrml\u001b[38;5;241m.\u001b[39mtrain() \u001b[38;5;66;03m# prep model for training\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m train_loader_rnn:\n\u001b[0;32m     20\u001b[0m   \u001b[38;5;66;03m# transfer data and target to GPU\u001b[39;00m\n\u001b[0;32m     21\u001b[0m   data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     22\u001b[0m   \u001b[38;5;66;03m# clear the gradients of all optimized variables\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:143\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    140\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:143\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    140\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:120\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 120\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:170\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# array of string classes and object\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m collate([torch\u001b[38;5;241m.\u001b[39mas_tensor(b) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found object"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_rnn_nrml.train() # prep model for training\n",
    "  for data, target in train_loader_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_rnn.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_rnn_nrml(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_rnn.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_rnn_nrml.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_rnn:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_rnn_nrml(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_rnn.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_rnn.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_rnn_nrml.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obbF9EMf8x46"
   },
   "outputs": [],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_rnn_nrml.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "00FTZLh19OAo"
   },
   "outputs": [],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_rnn:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_rnn_nrml.to(device)\n",
    "    outputs = model_rnn_nrml(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy_simple-RNN-model_pretrained-word2vec_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KNRzsZ0eCZKs"
   },
   "source": [
    "**Part 5b a Gated Recurrent Unit (GRU) cell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ySGsJqrg9UhU"
   },
   "outputs": [],
   "source": [
    "train_data_GRU=train_data_rnn\n",
    "test_data_GRU=test_data_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lRrX7kty-m0E"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_GRU)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_GRU = torch.utils.data.DataLoader(train_data_GRU, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_GRU = torch.utils.data.DataLoader(train_data_GRU, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_GRU= torch.utils.data.DataLoader(test_data_GRU, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EYBVzTZQDbWk"
   },
   "outputs": [],
   "source": [
    "# define the GRU architecture\n",
    "class GRU(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "    super().__init__()\n",
    "    # Number of hidden dimensions\n",
    "    self.hidden_dim = hidden_dim\n",
    "    # Number of hidden layers\n",
    "    self.layer_dim = layer_dim\n",
    "    # GRU\n",
    "    self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "    # Output layer\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Initialize hidden state with zeros\n",
    "    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "    # One time step\n",
    "    out, hn = self.gru(x, h0)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GVh0ggpDwzm"
   },
   "outputs": [],
   "source": [
    "model_GRU = GRU(300, 20, 1, 5)\n",
    "print(model_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7eAazIPeEYEC"
   },
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "\n",
    "optimizer_GRU = torch.optim.SGD(model_GRU.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TghVeJ1_EDvz"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_GRU.train() # prep model for training\n",
    "  for data, target in train_loader_GRU:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_GRU.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_GRU(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_GRU.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_GRU.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_GRU:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_GRU(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_GRU.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_GRU.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_GRU.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSni30qnEump"
   },
   "outputs": [],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_GRU.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x9s1mtcFwQ0"
   },
   "outputs": [],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_GRU:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_GRU.to(device)\n",
    "    outputs = model_GRU(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy_simple-RNN-model_pretrained-word2vec_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SoMBTjiG5QZ"
   },
   "source": [
    "**Part 5c) LSTM network **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePmkflEVHB-B"
   },
   "outputs": [],
   "source": [
    "train_data_LSTM=train_data_rnn\n",
    "test_data_LSTM=test_data_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dRR-MdqNF5_O"
   },
   "outputs": [],
   "source": [
    "# class LSTM(nn.Module):\n",
    "#     def __init__(self, input_size, output_size, hidden_dim, n_layers, model_type = \"lstm\"):\n",
    "#         super(LSTM, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.n_layers = n_layers\n",
    "#         self.model_type = model_type\n",
    "#         self.layer = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(400, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.size(0)\n",
    "#         hidden = self.init_hidden(batch_size)\n",
    "#         out, hidden = self.layer(x, hidden)\n",
    "#         out = out.contiguous().view(-1, out.shape[1] * out.shape[2])\n",
    "#         out = self.fc(out)\n",
    "#         return out, hidden\n",
    "\n",
    "#     def init_hidden(self, batch_size):\n",
    "#         hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim),torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "#         return hidden\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "    super().__init__()\n",
    "    # Number of hidden dimensions\n",
    "    self.hidden_dim = hidden_dim\n",
    "    # Number of hidden layers\n",
    "    self.layer_dim = layer_dim\n",
    "    # GRU\n",
    "    self.LSTM = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "    # Output layer\n",
    "    self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Initialize hidden state with zeros\n",
    "   \n",
    "    h0 =  (torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device), torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device))\n",
    "    # One time step\n",
    "    out, hn = self.LSTM(x, h0)\n",
    "    out = self.fc(out[:, -1, :])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9Tnjc_iHXgU"
   },
   "outputs": [],
   "source": [
    "model_LSTM = LSTM(300, 20, 1, 4)\n",
    "print(model_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nWCK5igHgon"
   },
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_LSTM)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samples for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_LSTM = torch.utils.data.DataLoader(train_data_LSTM, batch_size=batch_size,\n",
    "                                           sampler=train_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "valid_loader_LSTM = torch.utils.data.DataLoader(train_data_LSTM, batch_size=batch_size,\n",
    "                                           sampler=valid_sampler, num_workers\n",
    "                                           =num_workers)\n",
    "test_loader_LSTM= torch.utils.data.DataLoader(test_data_LSTM, batch_size=batch_size,\n",
    "                                           num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OU6KQKQH3yD"
   },
   "outputs": [],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "\n",
    "optimizer_LSTM = torch.optim.SGD(model_LSTM.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jJfWQgKIN8R"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  # monitor training loss\n",
    "  train_loss = 0.0\n",
    "  valid_loss = 0.0\n",
    "\n",
    "  ###################\n",
    "  # train the model #\n",
    "  ###################\n",
    "  model_LSTM.train() # prep model for training\n",
    "  for data, target in train_loader_LSTM:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # clear the gradients of all optimized variables\n",
    "    optimizer_LSTM.zero_grad()\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_LSTM(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # backward pass: compute gradient of the loss with respect to model parameters\n",
    "    loss.backward()\n",
    "    # perform a single optimization step (parameter update)\n",
    "    optimizer_LSTM.step()\n",
    "    # update running training loss\n",
    "    train_loss += loss.item()*data.size(0)\n",
    "\n",
    "  ######################\n",
    "  # validate the model #\n",
    "  ######################\n",
    "  model_LSTM.eval() # prep model for evaluation\n",
    "  for data, target in valid_loader_LSTM:\n",
    "    # transfer data and target to GPU\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    # forward pass: compute predicted outputs by passing inputs to the model\n",
    "    output = model_LSTM(data.float())\n",
    "    # calculate the loss\n",
    "    loss = criterion(output, target)\n",
    "    # update running validation loss\n",
    "    valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "\n",
    "  # print training/validation statistics\n",
    "  # calculate average loss over an epoch\n",
    "  train_loss = train_loss/len(train_loader_LSTM.dataset)\n",
    "  valid_loss = valid_loss/len(valid_loader_LSTM.dataset)\n",
    "\n",
    "  print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, train_loss, valid_loss))\n",
    "    \n",
    "  # save model if validation loss has decreased\n",
    "  if valid_loss <= valid_loss_min:\n",
    "    print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'\n",
    "        .format(valid_loss_min, valid_loss))\n",
    "    torch.save(model_LSTM.state_dict(), 'model.pt')\n",
    "    valid_loss_min = valid_loss\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvDhT7vwIxZg"
   },
   "outputs": [],
   "source": [
    "# Load the model with the lowest validation loss\n",
    "model_LSTM.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFbhW_xhNxGM"
   },
   "outputs": [],
   "source": [
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for out outputs\n",
    "with torch.no_grad():\n",
    "  for data in test_loader_LSTM:\n",
    "    embeddings, labels = data\n",
    "    # transfer data and target to GPU\n",
    "    embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "    # calculating outputs by running embeddings through the network\n",
    "    model_LSTM.to(device)\n",
    "    outputs = model_LSTM(embeddings.float())\n",
    "    # the class with the highest score is what we choose as prediction\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy_simple-LSTM-model_pretrained-word2vec_ternary-test-set: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ctjsjrUN6bT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPciZjTSKHslzu8Ti+yQXOK",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
